---
date: '2025-02-04T13:43:41+08:00'
draft: true
title: 'GPT and Scaling'
math: true
ShowReadingTime: false
ShowWordCount: false
---

| 模型 | GPT1 | GPT2 | GPT3 |
| :-----: | :-----: | :-----: | :-----: |
| 参数量| 0.1B | 1.5B | 175B |

## GPT 1.0 & Pretraining

* Pretraining+Fine Tuning的范式
* Decoder-only的框架

## GPT 2.0 & Parallelism

Pre-LN

## GPT 3.0 & Scaling Law

## References

- [Awesome LLM](https://github.com/Hannibal046/Awesome-LLM?tab=readme-ov-file)
- GPT 1.0: [Improving Language Understanding
by Generative Pre-Training](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)
- GPT 2.0: [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- Megatron-LM: [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053)
- Scaling Law: [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361)
- GPT 3.0: [Language models are few-shot learners](https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)
- [Physics of Language Models](https://physics.allen-zhu.com/home)